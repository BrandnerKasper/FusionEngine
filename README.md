# Project Generative Neural Rendering (Work in Progress)

[Deep Learning Super Sampling (DLSS) 4 - Multi Frame Generation](https://www.youtube.com/watch?v=qQn3bsPNTyI&t=7s)
elevates Neural Rendering to a new level combining super resolution (times 2 per axis) + 3 generated frames in between each frame
15/16 pixels are generated by AI. This equals to **93.75%** of what is shown to the player!

But what if we can push this up to 100%?
What if we do not need to render the game traditionally any longer?

With this project I want to explore this idea.

## Idea

Other work in this field like [Game'n'Gen](https://arxiv.org/abs/2408.14837) and [PlayGen](https://arxiv.org/abs/2412.00887)
utilizes a stable diffusion network and feed the game state as semantic vector to generate the game.

I propose a different idea, what if the game is internally rendered in a simplified version like ASCII and works completely on their own.
We then use the simplified visualization and feed that into a network to generate the complex visualization.

Here is an example:
![idea](img/idea.png)

Instead of abstracting the game and its logic and let the diffusion network embeed (and therefore learn) the game.
We only use it as a different visualization method and let the game engine still handle the logic.

## Prerequisites

Since this is a complex project, I need to check which components we need to complete a prototype.
As this is a WIP I will update this ReadMe document once I gathered more information or stuff got done.

A TODO list follows.

### TODO:

- [x] Checkout Pytorch CPP so we can use neural networks in a Cpp project
- [x] Write a simple ASCII game
- [x] Write a renderer in OpenGL/Vulcan to visualize the game differently
- [x] Generate a dataset of ASCII to PNG (for now small!)
- [ ] Find a suitable neural network architecture
- [ ] Hook the trained neural network into the cpp application and actually play via a neural engine!